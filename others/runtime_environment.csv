spark.driver.extraClassPath,(none),"Extra classpath entries to prepend to the classpath of the driver. 
Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-class-path command line option or in your default properties file.",2
spark.driver.extraJavaOptions,(none),"A string of extra JVM options to pass to the driver. For instance, GC settings or other logging. Note that it is illegal to set maximum heap size (-Xmx) settings with this option. Maximum heap size settings can be set with spark.driver.memory in the cluster mode and through the --driver-memory command line option in the client mode. 
Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-java-options command line option or in your default properties file.",2
spark.driver.extraLibraryPath,(none),"Set a special library path to use when launching the driver JVM. 
Note: In client mode, this config must not be set through the SparkConf directly in your application, because the driver JVM has already started at that point. Instead, please set this through the --driver-library-path command line option or in your default properties file.",2
spark.driver.userClassPathFirst,FALSE,(Experimental) Whether to give user-added jars precedence over Spark's own jars when loading classes in the driver. This feature can be used to mitigate conflicts between Spark's dependencies and user dependencies. It is currently an experimental feature. This is used in cluster mode only.,2
spark.executor.extraClassPath,(none),Extra classpath entries to prepend to the classpath of executors. This exists primarily for backwards-compatibility with older versions of Spark. Users typically should not need to set this option.,2
spark.executor.extraJavaOptions,(none),"A string of extra JVM options to pass to executors. For instance, GC settings or other logging. Note that it is illegal to set Spark properties or maximum heap size (-Xmx) settings with this option. Spark properties should be set using a SparkConf object or the spark-defaults.conf file used with the spark-submit script. Maximum heap size settings can be set with spark.executor.memory.",2
spark.executor.extraLibraryPath,(none),Set a special library path to use when launching executor JVM's.,2
spark.executor.logs.rolling.maxRetainedFiles,(none),Sets the number of latest rolling log files that are going to be retained by the system. Older log files will be deleted. Disabled by default.,2
spark.executor.logs.rolling.enableCompression,FALSE,"Enable executor log compression. If it is enabled, the rolled executor logs will be compressed. Disabled by default.",2
spark.executor.logs.rolling.maxSize,(none),Set the max size of the file in bytes by which the executor logs will be rolled over. Rolling is disabled by default. See spark.executor.logs.rolling.maxRetainedFiles for automatic cleaning of old logs.,2
spark.executor.logs.rolling.strategy,(none),"Set the strategy of rolling of executor logs. By default it is disabled. It can be set to ""time"" (time-based rolling) or ""size"" (size-based rolling). For ""time"", use spark.executor.logs.rolling.time.interval to set the rolling interval. For ""size"", use spark.executor.logs.rolling.maxSize to set the maximum file size for rolling.",2
spark.executor.logs.rolling.time.interval,daily,"Set the time interval by which the executor logs will be rolled over. Rolling is disabled by default. Valid values are daily, hourly, minutely or any interval in seconds. See spark.executor.logs.rolling.maxRetainedFiles for automatic cleaning of old logs.",2
spark.executor.userClassPathFirst,FALSE,"(Experimental) Same functionality as spark.driver.userClassPathFirst, but applied to executor instances.",2
spark.executorEnv.[EnvironmentVariableName],(none),Add the environment variable specified by EnvironmentVariableName to the Executor process. The user can specify multiple of these to set multiple environment variables.,2
spark.redaction.regex,(?i)secret|password,"Regex to decide which Spark configuration properties and environment variables in driver and executor environments contain sensitive information. When this regex matches a property key or value, the value is redacted from the environment UI and various logs like YARN and event logs.",2
spark.python.profile,FALSE,"Enable profiling in Python worker, the profile result will show up by sc.show_profiles(), or it will be displayed before the driver exits. It also can be dumped into disk by sc.dump_profiles(path). If some of the profile results had been displayed manually, they will not be displayed automatically before driver exiting. By default the pyspark.profiler.BasicProfiler will be used, but this can be overridden by passing a profiler class in as a parameter to the SparkContext constructor.",2
spark.python.profile.dump,(none),"The directory which is used to dump the profile result before driver exiting. The results will be dumped as separated file for each RDD. They can be loaded by ptats.Stats(). If this is specified, the profile result will not be displayed automatically.",2
spark.python.worker.memory,512m,"Amount of memory to use per python worker process during aggregation, in the same format as JVM memory strings with a size unit suffix (""k"", ""m"", ""g"" or ""t"") (e.g. 512m, 2g). If the memory used during aggregation goes above this amount, it will spill the data into disks.",2
spark.python.worker.reuse,TRUE,"Reuse Python worker or not. If yes, it will use a fixed number of Python workers, does not need to fork() a Python process for every task. It will be very useful if there is large broadcast, then the broadcast will not be needed to transferred from JVM to Python worker for every task.",2
spark.files,,Comma-separated list of files to be placed in the working directory of each executor. Globs are allowed.,2
spark.submit.pyFiles,,"Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. Globs are allowed.",2
spark.jars,,Comma-separated list of jars to include on the driver and executor classpaths. Globs are allowed.,2
spark.jars.packages,,"Comma-separated list of Maven coordinates of jars to include on the driver and executor classpaths. The coordinates should be groupId:artifactId:version. If spark.jars.ivySettings is given artifacts will be resolved according to the configuration in the file, otherwise artifacts will be searched for in the local maven repo, then maven central and finally any additional remote repositories given by the command-line option --repositories. For more details, seeAdvanced Dependency Management.",2
spark.jars.excludes,,"Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in spark.jars.packages to avoid dependency conflicts.",2
spark.jars.ivy,,"Path to specify the Ivy user directory, used for the local Ivy cache and package files from spark.jars.packages. This will override the Ivy property ivy.default.ivy.user.dir which defaults to ~/.ivy2.",2
spark.jars.ivySettings,,"Path to an Ivy settings file to customize resolution of jars specified using spark.jars.packages instead of the built-in defaults, such as maven central. Additional repositories given by the command-line option --repositories or spark.jars.repositories will also be included. Useful for allowing Spark to resolve artifacts from behind a firewall e.g. via an in-house artifact server like Artifactory. Details on the settings file format can be found at http://ant.apache.org/ivy/history/latest-milestone/settings.html",2
spark.jars.repositories,,Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages or spark.jars.packages.,2
spark.pyspark.driver.python,,Python binary executable to use for PySpark in driver. (default is spark.pyspark.python),2
spark.pyspark.python,,Python binary executable to use for PySpark in both driver and executors.,2