spark.acls.enable,FALSE,"Whether Spark acls should be enabled. If enabled, this checks to see if the user has access permissions to view or modify the job. Note this requires the user to be known, so if the user comes across as null no checks are done. Filters can be used with the UI to authenticate and set the user.",11
spark.admin.acls,Empty,"Comma separated list of users/administrators that have view and modify access to all Spark jobs. This can be used if you run on a shared cluster and have a set of administrators or devs who help debug when things do not work. Putting a ""*"" in the list means any user can have the privilege of admin.",11
spark.admin.acls.groups,Empty,"Comma separated list of groups that have view and modify access to all Spark jobs. This can be used if you have a set of administrators or developers who help maintain and debug the underlying infrastructure. Putting a ""*"" in the list means any user in any group can have the privilege of admin. The user groups are obtained from the instance of the groups mapping provider specified by spark.user.groups.mapping. Check the entryspark.user.groups.mapping for more details.",11
spark.user.groups.mapping,org.apache.spark.security.ShellBasedGroupsMappingProvider,"The list of groups for a user is determined by a group mapping service defined by the trait org.apache.spark.security.GroupMappingServiceProvider which can be configured by this property. A default unix shell based implementation is provided org.apache.spark.security.ShellBasedGroupsMappingProviderwhich can be specified to resolve a list of groups for a user. Note:This implementation supports only a Unix/Linux based environment. Windows environment is currently not supported. However, a new platform/protocol can be supported by implementing the trait org.apache.spark.security.GroupMappingServiceProvider.",11
spark.authenticate,FALSE,Whether Spark authenticates its internal connections. Seespark.authenticate.secret if not running on YARN.,11
spark.authenticate.secret,None,Set the secret key used for Spark to authenticate between components. This needs to be set if not running on YARN and authentication is enabled.,11
spark.network.crypto.enabled,FALSE,Enable encryption using the commons-crypto library for RPC and block transfer service. Requires spark.authenticate to be enabled.,11
spark.network.crypto.keyLength,128,"The length in bits of the encryption key to generate. Valid values are 128, 192 and 256.",11
spark.network.crypto.keyFactoryAlgorithm,PBKDF2WithHmacSHA1,The key factory algorithm to use when generating encryption keys. Should be one of the algorithms supported by the javax.crypto.SecretKeyFactory class in the JRE being used.,11
spark.network.crypto.saslFallback,TRUE,"Whether to fall back to SASL authentication if authentication fails using Spark's internal mechanism. This is useful when the application is connecting to old shuffle services that do not support the internal Spark authentication protocol. On the server side, this can be used to block older clients from authenticating against a new shuffle service.",11
spark.network.crypto.config.*,None,"Configuration values for the commons-crypto library, such as which cipher implementations to use. The config name should be the name of commons-crypto configuration without the ""commons.crypto"" prefix.",11
spark.authenticate.enableSaslEncryption,FALSE,Enable encrypted communication when authentication is enabled. This is supported by the block transfer service and the RPC endpoints.,11
spark.network.sasl.serverAlwaysEncrypt,FALSE,Disable unencrypted connections for services that support SASL authentication.,11
spark.core.connection.ack.wait.timeout,spark.network.timeout,"How long for the connection to wait for ack to occur before timing out and giving up. To avoid unwilling timeout caused by long pause like GC, you can set larger value.",11
spark.modify.acls,Empty,"Comma separated list of users that have modify access to the Spark job. By default only the user that started the Spark job has access to modify it (kill it for example). Putting a ""*"" in the list means any user can have access to modify it.",11
spark.modify.acls.groups,Empty,"Comma separated list of groups that have modify access to the Spark job. This can be used if you have a set of administrators or developers from the same team to have access to control the job. Putting a ""*"" in the list means any user in any group has the access to modify the Spark job. The user groups are obtained from the instance of the groups mapping provider specified byspark.user.groups.mapping. Check the entry spark.user.groups.mapping for more details.",11
spark.ui.filters,None,"Comma separated list of filter class names to apply to the Spark web UI. The filter should be a standard javax servlet Filter. Parameters to each filter can also be specified by setting a java system property of: 
spark.<class name of filter>.params='param1=value1,param2=value2'
For example: 
-Dspark.ui.filters=com.test.filter1 
-Dspark.com.test.filter1.params='param1=foo,param2=testing'",11
spark.ui.view.acls,Empty,"Comma separated list of users that have view access to the Spark web ui. By default only the user that started the Spark job has view access. Putting a ""*"" in the list means any user can have view access to this Spark job.",11
spark.ui.view.acls.groups,Empty,"Comma separated list of groups that have view access to the Spark web ui to view the Spark Job details. This can be used if you have a set of administrators or developers or users who can monitor the Spark job submitted. Putting a ""*"" in the list means any user in any group can view the Spark job details on the Spark web ui. The user groups are obtained from the instance of the groups mapping provider specified by spark.user.groups.mapping. Check the entry spark.user.groups.mapping for more details.",11