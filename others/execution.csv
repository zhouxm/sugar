spark.broadcast.blockSize,4m,"Size of each piece of a block for TorrentBroadcastFactory, in KiB unless otherwise specified. Too large a value decreases parallelism during broadcast (makes it slower); however, if it is too small, BlockManager might take a performance hit.",7
spark.executor.cores,"1 in YARN mode, all the available cores on the worker in standalone and Mesos coarse-grained modes.","The number of cores to use on each executor. In standalone and Mesos coarse-grained modes, for more detail, see this description.",7
spark.default.parallelism,"For distributed shuffle operations like reduceByKeyand join, the largest number of partitions in a parent RDD. For operations like parallelizewith no parent RDDs, it depends on the cluster manager:
Local mode: number of cores on the local machine
Mesos fine grained mode: 8
Others: total number of cores on all executor nodes or 2, whichever is larger","Default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set by user.",7
spark.executor.heartbeatInterval,10s,Interval between each executor's heartbeats to the driver. Heartbeats let the driver know that the executor is still alive and update it with metrics for in-progress tasks. spark.executor.heartbeatInterval should be significantly less than spark.network.timeout,7
spark.files.fetchTimeout,60s,Communication timeout to use when fetching files added through SparkContext.addFile() from the driver.,7
spark.files.useFetchCache,TRUE,"If set to true (default), file fetching will use a local cache that is shared by executors that belong to the same application, which can improve task launching performance when running many executors on the same host. If set to false, these caching optimizations will be disabled and all executors will fetch their own copies of files. This optimization may be disabled in order to use Spark local directories that reside on NFS filesystems (see SPARK-6313 for more details).",7
spark.files.overwrite,FALSE,Whether to overwrite files added through SparkContext.addFile() when the target file exists and its contents do not match those of the source.,7
spark.files.maxPartitionBytes,134217728 (128 MB),The maximum number of bytes to pack into a single partition when reading files.,7
spark.files.openCostInBytes,4194304 (4 MB),"The estimated cost to open a file, measured by the number of bytes could be scanned at the same time. This is used when putting multiple files into a partition. It is better to over estimate, then the partitions with small files will be faster than partitions with bigger files.",7
spark.hadoop.cloneConf,FALSE,"If set to true, clones a new Hadoop Configurationobject for each task. This option should be enabled to work around Configuration thread-safety issues (see SPARK-2546 for more details). This is disabled by default in order to avoid unexpected performance regressions for jobs that are not affected by these issues.",7
spark.hadoop.validateOutputSpecs,TRUE,"If set to true, validates the output specification (e.g. checking if the output directory already exists) used in saveAsHadoopFile and other variants. This can be disabled to silence exceptions due to pre-existing output directories. We recommend that users do not disable this except if trying to achieve compatibility with previous versions of Spark. Simply use Hadoop's FileSystem API to delete output directories by hand. This setting is ignored for jobs generated through Spark Streaming's StreamingContext, since data may need to be rewritten to pre-existing output directories during checkpoint recovery.",7
spark.storage.memoryMapThreshold,2m,"Size in bytes of a block above which Spark memory maps when reading a block from disk. This prevents Spark from memory mapping very small blocks. In general, memory mapping has high overhead for blocks close to or below the page size of the operating system.",7
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version,1,"The file output committer algorithm version, valid algorithm version number: 1 or 2. Version 2 may have better performance, but version 1 may handle failures better in certain situations, as per MAPREDUCE-4815.",7