spark.cores.max,(not set),"When running on a standalone deploy cluster or a Mesos cluster in ""coarse-grained"" sharing mode, the maximum amount of CPU cores to request for the application from across the cluster (not from each machine). If not set, the default will be spark.deploy.defaultCores on Spark's standalone cluster manager, or infinite (all available cores) on Mesos.",9
spark.locality.wait,3s,"How long to wait to launch a data-local task before giving up and launching it on a less-local node. The same wait will be used to step through multiple locality levels (process-local, node-local, rack-local and then any). It is also possible to customize the waiting time for each level by setting spark.locality.wait.node, etc. You should increase this setting if your tasks are long and see poor locality, but the default usually works well.",9
spark.locality.wait.node,spark.locality.wait,"Customize the locality wait for node locality. For example, you can set this to 0 to skip node locality and search immediately for rack locality (if your cluster has rack information).",9
spark.locality.wait.process,spark.locality.wait,Customize the locality wait for process locality. This affects tasks that attempt to access cached data in a particular executor process.,9
spark.locality.wait.rack,spark.locality.wait,Customize the locality wait for rack locality.,9
spark.scheduler.maxRegisteredResourcesWaitingTime,30s,Maximum amount of time to wait for resources to register before scheduling begins.,9
spark.scheduler.minRegisteredResourcesRatio,0.8 for KUBERNETES mode; 0.8 for YARN mode; 0.0 for standalone mode and Mesos coarse-grained mode,"The minimum ratio of registered resources (registered resources / total expected resources) (resources are executors in yarn mode and Kubernetes mode, CPU cores in standalone mode and Mesos coarsed-grained mode ['spark.cores.max' value is total expected resources for Mesos coarse-grained mode] ) to wait for before scheduling begins. Specified as a double between 0.0 and 1.0. Regardless of whether the minimum ratio of resources has been reached, the maximum amount of time it will wait before scheduling begins is controlled by configspark.scheduler.maxRegisteredResourcesWaitingTime.",9
spark.scheduler.mode,FIFO,The scheduling mode between jobs submitted to the same SparkContext. Can be set to FAIR to use fair sharing instead of queueing jobs one after another. Useful for multi-user services.,9
spark.scheduler.revive.interval,1s,The interval length for the scheduler to revive the worker resource offers to run tasks.,9
spark.scheduler.listenerbus.eventqueue.capacity,10000,"Capacity for event queue in Spark listener bus, must be greater than 0. Consider increasing value (e.g. 20000) if listener events are dropped. Increasing this value may result in the driver using more memory.",9
spark.blacklist.enabled,FALSE,"If set to ""true"", prevent Spark from scheduling tasks on executors that have been blacklisted due to too many task failures. The blacklisting algorithm can be further controlled by the other ""spark.blacklist"" configuration options.",9
spark.blacklist.timeout,1h,"(Experimental) How long a node or executor is blacklisted for the entire application, before it is unconditionally removed from the blacklist to attempt running new tasks.",9
spark.blacklist.task.maxTaskAttemptsPerExecutor,1,"(Experimental) For a given task, how many times it can be retried on one executor before the executor is blacklisted for that task.",9
spark.blacklist.task.maxTaskAttemptsPerNode,2,"(Experimental) For a given task, how many times it can be retried on one node, before the entire node is blacklisted for that task.",9
spark.blacklist.stage.maxFailedTasksPerExecutor,2,"(Experimental) How many different tasks must fail on one executor, within one stage, before the executor is blacklisted for that stage.",9
spark.blacklist.stage.maxFailedExecutorsPerNode,2,"(Experimental) How many different executors are marked as blacklisted for a given stage, before the entire node is marked as failed for the stage.",9
spark.blacklist.application.maxFailedTasksPerExecutor,2,"(Experimental) How many different tasks must fail on one executor, in successful task sets, before the executor is blacklisted for the entire application. Blacklisted executors will be automatically added back to the pool of available resources after the timeout specified byspark.blacklist.timeout. Note that with dynamic allocation, though, the executors may get marked as idle and be reclaimed by the cluster manager.",9
spark.blacklist.application.maxFailedExecutorsPerNode,2,"(Experimental) How many different executors must be blacklisted for the entire application, before the node is blacklisted for the entire application. Blacklisted nodes will be automatically added back to the pool of available resources after the timeout specified byspark.blacklist.timeout. Note that with dynamic allocation, though, the executors on the node may get marked as idle and be reclaimed by the cluster manager.",9
spark.blacklist.killBlacklistedExecutors,FALSE,"(Experimental) If set to ""true"", allow Spark to automatically kill, and attempt to re-create, executors when they are blacklisted. Note that, when an entire node is added to the blacklist, all of the executors on that node will be killed.",9
spark.blacklist.application.fetchFailure.enabled,FALSE,"(Experimental) If set to ""true"", Spark will blacklist the executor immediately when a fetch failure happenes. If external shuffle service is enabled, then the whole node will be blacklisted.",9
spark.speculation,FALSE,"If set to ""true"", performs speculative execution of tasks. This means if one or more tasks are running slowly in a stage, they will be re-launched.",9
spark.speculation.interval,100ms,How often Spark will check for tasks to speculate.,9
spark.speculation.multiplier,1.5,How many times slower a task is than the median to be considered for speculation.,9
spark.speculation.quantile,0.75,Fraction of tasks which must be complete before speculation is enabled for a particular stage.,9
spark.task.cpus,1,Number of cores to allocate for each task.,9
spark.task.maxFailures,4,Number of failures of any particular task before giving up on the job. The total number of failures spread across different tasks will not cause the job to fail; a particular task has to fail this number of attempts. Should be greater than or equal to 1. Number of allowed retries = this value - 1.,9
spark.task.reaper.enabled,FALSE,"Enables monitoring of killed / interrupted tasks. When set to true, any task which is killed will be monitored by the executor until that task actually finishes executing. See the other spark.task.reaper.* configurations for details on how to control the exact behavior of this monitoring. When set to false (the default), task killing will use an older code path which lacks such monitoring.",9
spark.task.reaper.pollingInterval,10s,"When spark.task.reaper.enabled = true, this setting controls the frequency at which executors will poll the status of killed tasks. If a killed task is still running when polled then a warning will be logged and, by default, a thread-dump of the task will be logged (this thread dump can be disabled via the spark.task.reaper.threadDumpsetting, which is documented below).",9
spark.task.reaper.threadDump,TRUE,"When spark.task.reaper.enabled = true, this setting controls whether task thread dumps are logged during periodic polling of killed tasks. Set this to false to disable collection of thread dumps.",9
spark.task.reaper.killTimeout,-1,"When spark.task.reaper.enabled = true, this setting specifies a timeout after which the executor JVM will kill itself if a killed task has not stopped running. The default value, -1, disables this mechanism and prevents the executor from self-destructing. The purpose of this setting is to act as a safety-net to prevent runaway uncancellable tasks from rendering an executor unusable.",9
spark.stage.maxConsecutiveAttempts,4,Number of consecutive stage attempts allowed before a stage is aborted.,9