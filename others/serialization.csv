spark.broadcast.compress,TRUE,Whether to compress broadcast variables before sending them. Generally a good idea. Compression will use spark.io.compression.codec.,5
spark.io.compression.codec,lz4,"The codec used to compress internal data such as RDD partitions, event log, broadcast variables and shuffle outputs. By default, Spark provides three codecs: lz4, lzf, and snappy. You can also use fully qualified class names to specify the codec, e.g.org.apache.spark.io.LZ4CompressionCodec,org.apache.spark.io.LZFCompressionCodec,org.apache.spark.io.SnappyCompressionCodec, and org.apache.spark.io.ZstdCompressionCodec.",5
spark.io.compression.lz4.blockSize,32k,"Block size in bytes used in LZ4 compression, in the case when LZ4 compression codec is used. Lowering this block size will also lower shuffle memory usage when LZ4 is used.",5
spark.io.compression.snappy.blockSize,32k,"Block size in bytes used in Snappy compression, in the case when Snappy compression codec is used. Lowering this block size will also lower shuffle memory usage when Snappy is used.",5
spark.io.compression.zstd.level,1,Compression level for Zstd compression codec. Increasing the compression level will result in better compression at the expense of more CPU and memory.,5
spark.io.compression.zstd.bufferSize,32k,"Buffer size in bytes used in Zstd compression, in the case when Zstd compression codec is used. Lowering this size will lower the shuffle memory usage when Zstd is used, but it might increase the compression cost because of excessive JNI call overhead.",5
spark.kryo.classesToRegister,(none),"If you use Kryo serialization, give a comma-separated list of custom class names to register with Kryo. See the tuning guide for more details.",5
spark.kryo.referenceTracking,TRUE,"Whether to track references to the same object when serializing data with Kryo, which is necessary if your object graphs have loops and useful for efficiency if they contain multiple copies of the same object. Can be disabled to improve performance if you know this is not the case.",5
spark.kryo.registrationRequired,FALSE,"Whether to require registration with Kryo. If set to 'true', Kryo will throw an exception if an unregistered class is serialized. If set to false (the default), Kryo will write unregistered class names along with each object. Writing class names can cause significant performance overhead, so enabling this option can enforce strictly that a user has not omitted classes from registration.",5
spark.kryo.registrator,(none),"If you use Kryo serialization, give a comma-separated list of classes that register your custom classes with Kryo. This property is useful if you need to register your classes in a custom way, e.g. to specify a custom field serializer. Otherwise spark.kryo.classesToRegister is simpler. It should be set to classes that extend KryoRegistrator. See the tuning guide for more details.",5
spark.kryo.unsafe,FALSE,Whether to use unsafe based Kryo serializer. Can be substantially faster by using Unsafe Based IO.,5
spark.kryoserializer.buffer.max,64m,"Maximum allowable size of Kryo serialization buffer, in MiB unless otherwise specified. This must be larger than any object you attempt to serialize and must be less than 2048m. Increase this if you get a ""buffer limit exceeded"" exception inside Kryo.",5
spark.kryoserializer.buffer,64k,"Initial size of Kryo's serialization buffer, in KiB unless otherwise specified. Note that there will be one buffer per core on each worker. This buffer will grow up tospark.kryoserializer.buffer.max if needed.",5
spark.rdd.compress,FALSE,Whether to compress serialized RDD partitions (e.g. forStorageLevel.MEMORY_ONLY_SER in Java and Scala or StorageLevel.MEMORY_ONLY in Python). Can save substantial space at the cost of some extra CPU time. Compression will use spark.io.compression.codec.,5
spark.serializer,"org.apache.spark.serializer.
JavaSerializer","Class to use for serializing objects that will be sent over the network or need to be cached in serialized form. The default of Java serialization works with any Serializable Java object but is quite slow, so we recommend usingorg.apache.spark.serializer.KryoSerializer and configuring Kryo serialization when speed is necessary. Can be any subclass oforg.apache.spark.Serializer.",5
spark.serializer.objectStreamReset,100,"When serializing using org.apache.spark.serializer.JavaSerializer, the serializer caches objects to prevent writing redundant data, however that stops garbage collection of those objects. By calling 'reset' you flush that info from the serializer, and allow old objects to be collected. To turn off this periodic reset set it to -1. By default it will reset the serializer every 100 objects.",5
spark.broadcast.compress,TRUE,Whether to compress broadcast variables before sending them. Generally a good idea. Compression will use spark.io.compression.codec.,5
spark.io.compression.codec,lz4,"The codec used to compress internal data such as RDD partitions, event log, broadcast variables and shuffle outputs. By default, Spark provides three codecs: lz4, lzf, and snappy. You can also use fully qualified class names to specify the codec, e.g.org.apache.spark.io.LZ4CompressionCodec,org.apache.spark.io.LZFCompressionCodec,org.apache.spark.io.SnappyCompressionCodec, and org.apache.spark.io.ZstdCompressionCodec.",5
spark.io.compression.lz4.blockSize,32k,"Block size in bytes used in LZ4 compression, in the case when LZ4 compression codec is used. Lowering this block size will also lower shuffle memory usage when LZ4 is used.",5
spark.io.compression.snappy.blockSize,32k,"Block size in bytes used in Snappy compression, in the case when Snappy compression codec is used. Lowering this block size will also lower shuffle memory usage when Snappy is used.",5